---
title: "Classification Script"
author: "Alex Fennell"
output: html_document
---

# Synopsis
The goal of this project is to correctly classify the quality of physical activity an individual
carried out based on accelerometer collected from the belt, forearm, arm, and dumbbell.
There were 5 different ways in which the exercise was carried out. It was done either 
exactly according to the specification (Class A), throwing the elbows to the front (Class B),
lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) 
and throwing the hips to the front (Class E). Class A is the correct manner in which
the exercise should be carried out, while the other classes are common errors. 
Using the accelerometer data I was able to create a random forest model that classified
this information into the 5 desired classes with 98 percent accuracy.


```{r libraries,message=FALSE,warning=FALSE}
library(Hmisc)
library(vip)
library(dplyr)
library(randomForest)
library(e1071)
library(parallel)
library(MLmetrics)
library(foreach)
library(doParallel)
library(caret)
```

# Read in the Data

```{r data loading}
#train data location
fileurl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
if (!file.exists("fittrain.csv")){
    download.file(fileurl,"fittrain.csv",method="curl")
}
#test data location
fileurl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if (!file.exists("fittest.csv")){
    download.file(fileurl,"fittest.csv",method="curl")
}

# The data sets had many entries that were spaces ("") and thus I specified these
# to be missing values (NA)
fit_train<-read.csv("fittrain.csv",na.strings = c("NA",""))
fit_test<-read.csv("fittest.csv",na.strings = c("NA",""))
```

# Assessing missing data
The first thing I want to do is assess the amount of missing data, so I know if there
are certain predictors I should remove, or I should implement an imputation technique.

```{r examine missing data}
colSums(is.na(fit_train))
```

Given a good number of these predictors are columns of missing values, I will go 
ahead and remove any predictor that is more than 95% missing values.

## Removing Missing Values

```{r remove missing values}
threshold=.95
trainfilt<-fit_train[,colSums(is.na(fit_train))<(nrow(fit_train)*threshold)]
testfilt<-fit_test[,colSums(is.na(fit_test))<(nrow(fit_test)*threshold)]
```

## Keeping accelerometer data
I am only interested in data from the accelerometers, so I will remove columns that contain other information.

```{r remove timeseries data}
trainfilt<-trainfilt[,-c(1,2,3,4,5,6,7)]
testfilt<-testfilt[,-c(1,2,3,4,5,6,7)]

trainfilt$classe<-as.factor(trainfilt$classe)
testfilt$classe<-as.factor(testfilt$classe)

```

## Correlated Predictors
The next step in understanding the data is to look at correlations among
the predictors. If variables are highly related to each other, it may
be worthwhile to remove them or do some dimensionality reduction such
as PCA. I use a correlation of .75 here so as to only examine the most
highly correlated predictors.

```{r Find multicolinearity}
cormat<-trainfilt%>%
    select_if(is.numeric)%>%
    as.matrix()%>%
    rcorr(type='spearman')
highcor<-findCorrelation(cormat$r,cutoff = 0.75)
colnames(cormat$r[highcor,highcor])
```

It is not surprising that there are many correlated predictors given
that these are complex movements that require coordination from many
parts of the body. Given this, it seems inappropriate to remove
variables. Therefore I will go forward with a PCA technique in order to
reduce the dimensionality of the data while still retaining the most
informative aspects of the data.

## Near Zero Variance
Next I will examine the data set to determine if there are any uniformative
predictors and remove them if that is the case.

```{r determine near zero variance}
nsv<-nearZeroVar(trainfilt,saveMetrics = TRUE)
data.frame(zeroVar=sum(nsv$zeroVar==TRUE), NSV=sum(nsv$nzv==TRUE))

```

There are no uniformative predictors, so the data is ready for modelling.

## Validation data split
Before modelling, I split the training data set to include a validation
set so the model does not overfit.

```{r prepare data for modelling}
set.seed(1234)
samp<-createDataPartition(y=trainfilt$classe,p=.8,list = FALSE)
training<-trainfilt[samp,]
validation<-trainfilt[-samp,]
```

# Model analysis
The model I am going to use is a random forest model since these typically
have superior performance when it comes to classification. I am using a
repeated cross validation procedure with 10 folds, and 3 repeats to get
a stable model that minimizes overtraining. I will do a grid search to find
the optimal value for the mtry parameter. This parameter corresponds to the
number of variables randomly sampled as candidates at each split. I 
center and scale all the predictors and then do a pca selecting the
components that account for 95% of the variance. I use a tree size of
250 as this is enough to produce stable accuracy and is not overly
computationally intensive. Model performance will be assessed on its
out of sample error rate which is equivalent to 1-Accuracy.

## Data Preprocessing
```{r random forest model preprocessing}
control<-trainControl(method='repeatedcv',
                      number=10,
                      repeats = 3,
                      classProbs = TRUE,
                      summaryFunction = multiClassSummary,
                      allowParallel = TRUE,
                      savePredictions = TRUE,
                      search='grid',
                      verboseIter = TRUE)
tunevals<-expand.grid(.mtry=c(1:10))
#preprocess all datasets
trainpre<-preProcess(training,method=c('center','scale','pca'),thresh=.95)
trainpca<-predict(trainpre,training)
valpca<-predict(trainpre,validation)
testpca<-predict(trainpre,testfilt)
```

## Random Forest Model Fit
```{r random forest model fit,cache=TRUE,cache.lazy=TRUE}
#Parellelize the random forest process
cluster<-makeCluster(detectCores()-6) 
registerDoParallel(cluster)
set.seed(1234)
    rfmod<-train(classe~.,
                 data=trainpca,
                 method='rf',
                 ntree=250,
                 tuneGrid=tunevals,
                 verbose=TRUE,
                 metric='Accuracy',
                 trControl=control)

stopCluster(cluster)
registerDoSEQ()
```

```{r save rf model, echo=FALSE,eval=FALSE}
saveRDS(rfmod,file="RFmod.rds")
```

```{r load RF model, echo=FALSE}
rfmod<-readRDS("RFmod.rds")
```
## Optimal mtry 
This plot shows how the mtry value affects the cross validation accuracy
for the model.

```{r plot of parameter optimization}
plot(rfmod)
```

This plot shows that an mtry value of 3 results in the highest accuracy
on the held out cross validation sets.

# Variable Importance
Using the vip plot we can assess what the most valuable predictors are.

```{r vip plot}
vip(rfmod)
```

The most important predictor appears to be PC8, so let's examine that to
see what this component is capturing.

## Principal Component Analysis
```{r examination of pc 8}
a<-head(sort(trainpre$rotation[,8]),5)
b<-head(sort(trainpre$rotation[,8],decreasing=TRUE),5)
data.frame(PCAmag=c(a,b))
```

The strongest predictors that were combined in the PC8 component are
presented in the table above. This component is heavily influenced by 
gyroscopic information from the dumbbell, arm and forearm in mainly the
x and y planes. There is also some influence from the gyroscopic information
of the belt in the z direction. This makes sense that this would be the most
important component. Exercise mistakes were based on partial bicep curls, or
the involvement of the hips in the curl. Thus it should be expected that a
component that is dominated by information from the arm/forearm/dumbbell
and some from the belt should be the most informative in classifying the
quality of the exercise.

# Model Fit Evaluation
## Confusion matrix-caret cross validation data set
To assess the model fit I will be using confusion matrices. First I will
examine the model fit to the held out re-samples in the caret cross
validation procedure. Then I will examine the model performance against the 
validation set that I set aside before beginning the modelling.

```{r confusion matrix cross val holdout set}
confusionMatrix(rfmod)
```

On the cross validation set within the caret procedure, the model is 
quite accurate with few observations off the diagonals. 

## Confusion matrix-validation data set

```{r confusion matrix validation data set}
confusionMatrix(predict(rfmod,valpca),valpca$classe)
```

```{r model predictions test set}
rfpred<-predict(rfmod,testpca)
# Calculate accuracy on the test set
sum(rfpred==testfilt$classe)/nrow(testfilt)*100
```
# Conclusion 
With the validation set the model performs very well with an accuracy of 
~98%. Thus the model has an out of sample error of 2 percent. Given the small size
of the test set, the model performs with 100 percent accuracy. Thus this accelerometer
data serves as an excellent source of information to classify the quality of an exercise.


