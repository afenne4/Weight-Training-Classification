---
title: "Weight Lifting Analysis"
author: "Alex Fennell"
output: 
    html_document:
    toc: true
self_contained: true
github_document:
    toc: true
---
# Synopsis
The goal of this project is to correctly classify the quality of physical activity an individual
carried out based on accelerometer collected from the belt, forearm, arm, and dumbell.
There were 5 different ways in which the exercise was carried out. It was done either 
exactly according to the specification (Class A), throwing the elbows to the front (Class B),
lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) 
and throwing the hips to the front (Class E). Class A is the correct manner in which
the exercise should be carried out, while the other classes are common errors. 
Using the accelerometer data I was able to create a random forest model that classified
this inoformation into the 5 desired classes with 98 percent accuracy.


```{r libraries,message=FALSE,warning=FALSE}
library(Hmisc)
library(vip)
library(dplyr)
library(randomForest)
library(e1071)
library(parallel)
library(MLmetrics)
library(foreach)
library(doParallel)
library(caret)
```

# Read in the Data

```{r data loading}
#train data location
fileurl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
if (!file.exists("fittrain.csv")){
    download.file(fileurl,"fittrain.csv",method="curl")
}
#test data location
fileurl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if (!file.exists("fittest.csv")){
    download.file(fileurl,"fittest.csv",method="curl")
}

# The data sets had many entries that were spaces ("") and thus I specified these
# to be missing values (NA)
fit_train<-read.csv("fittrain.csv",na.strings = c("NA",""))
fit_test<-read.csv("fittest.csv",na.strings = c("NA",""))
```

# Assessing missing data
The first thing I want to do is assess the amount of missing data, so I know if there
are certain predictors I should remove, or I should implement an imputation technique.

```{r examine missing data}
colSums(is.na(fit_train))
```

Given a good number of these predictors are columns of missing values, I will go 
ahead and remove any predictor that is more than 95% missing values.

## Removing Missing Values

```{r remove missing values}
threshold=.95
trainfilt<-fit_train[,colSums(is.na(fit_train))<(nrow(fit_train)*threshold)]
testfilt<-fit_test[,colSums(is.na(fit_test))<(nrow(fit_test)*threshold)]
```

## Keeping accelerometer data
I am only interested in data from the accelerometers, so I will remove columns that contain other information.

```{r remove timeseries data}
trainfilt<-trainfilt[,-c(1,2,3,4,5,6,7)]
testfilt<-testfilt[,-c(1,2,3,4,5,6,7)]

trainfilt$classe<-as.factor(trainfilt$classe)
```

## Correlated Predictors
The next step in understanding the data is to look at correlations among
the predictors. If variables are highly related to each other, it may
be worthwhile to remove them or do some dimensionality reduction such
as PCA. I use a correlation of .75 here so as to only examine the most
highly correlated predictors.

```{r Find multicolinearity}
cormat<-trainfilt%>%
    select_if(is.numeric)%>%
    as.matrix()%>%
    rcorr(type='spearman')
highcor<-findCorrelation(cormat$r,cutoff = 0.75)
colnames(cormat$r[highcor,highcor])
```

It is not surprising that there are many correlated predictors given
that these are complex movements that require coordination from many
parts of the body. Given this, it seems inappropriate to remove
variables. Therefore I will go forward with a PCA technique in order to
reduce the dimensionality of the data while still retaining the most
informative aspects of the data.

## Near Zero Variance
Next I will examine the data set to determine if there are any uniformative
predictors and remove them if that is the case.

```{r determine near zero variance}
nsv<-nearZeroVar(trainfilt,saveMetrics = TRUE)
data.frame(zeroVar=sum(nsv$zeroVar==TRUE), NSV=sum(nsv$nzv==TRUE))

```

There are no uniformative predictors, so the data is ready for modelling.

## Validation data split
Before modelling, I split the training data set to include a validation
set so the model does not overfit.

```{r prepare data for modelling}
samp<-createDataPartition(y=trainfilt$classe,p=.8,list = FALSE)
training<-trainfilt[samp,]
validation<-trainfilt[-samp,]
```

# Model analysis
The model I am going to use is a random forest model since these typically
have superior performance when it comes to classification. I am using a
repeated cross validation procedure with 10 folds, and 3 repeats to get
a stable model that minimizes overtraining. I will do a grid search to find
the optimal value for the mtry parameter. This parameter corresponds to the
number of variables randomly sampled as candidates at each split. I 
center and scale all the predictors and then do a pca selecting the
components that account for 95% of the variance. I use a tree size of
250 as this is enough to produce stable accuracy and is not overly
computationally intensive.

```{r random forest model fit,cache=TRUE}
control<-trainControl(method='repeatedcv',
                      number=10,
                      repeats = 3,
                      classProbs = TRUE,
                      summaryFunction = multiClassSummary,
                      allowParallel = TRUE,
                      savePredictions = TRUE,
                      search='grid',
                      verboseIter = TRUE)
tunevals<-expand.grid(.mtry=c(1:10))
#preprocess all datasets
trainpre<-preProcess(training,method=c('center','scale','pca'),thresh=.95)
trainpca<-predict(trainpre,training)
valpca<-predict(trainpre,validation)
testpca<-predict(trainpre,testfilt)

#Parellelize the random forest process
cluster<-makeCluster(detectCores()-6) 
registerDoParallel(cluster)
set.seed(1234)
    rfmod<-train(classe~.,
                 data=trainpca,
                 method='rf',
                 ntree=250,
                 tuneGrid=tunevals,
                 verbose=TRUE,
                 metric='Accuracy',
                 trControl=control)

stopCluster(cluster)
registerDoSEQ()
```
## Optimal mtry 
This plot shows how the mtry value affects the cross validation accuracy
for the model.

```{r plot of parameter optimization}
plot(rfmod)
```


# Variable Importance
Using the vip plot we can assess what the most valuable predictors are.

```{r vip plot}
vip(rfmod)
```

The most important predictor appears to be PC15, so let's examine that to
see what this component is capturing.

## Principal Component Analysis
```{r examination of pc 15}
a<-head(sort(trainpre$rotation[,15]),5)
b<-head(sort(trainpre$rotation[,15],decreasing=TRUE),5)
data.frame(PCAmag=c(a,b))
```

The strongest predictors that were combined in the PC15 component are
presented in the table above. This component is heavily influenced by 
gyroscopic information from all the belt directions in addition to the
magnitude from the belt. The component is also heavily influenced by the 
forearm acceleration and magnitude in the z and y planes. This makes sense
that this would be the most important component. Exercise mistakes were 
based on partial bicep curls, or the involvement of the hips in the curl.
Thus it should be expected that a component that is dominanted by both
pieces of information should be the most informative in classifying the 
quality of the exercise.


# Model Fit Evaluation
## Confusion matrix-caret cross validation data set
To assess the model fit I will be using confusion matrices. First I will
examine the model fit to the held out resamples in the caret cross
validation procedure. Then I will examine the model performance against the 
validation set that I set aside before beginning the modelling.

```{r confusion matrix cross val holdout set}
confusionMatrix(rfmod)
```

On the cross validation set within the caret procedure, the model is 
quite accurate with few observations off the diagonals. 

## Confusion matrix-validation data set

```{r confusion matrix validation data set}
confusionMatrix(predict(rfmod,valpca),valpca$classe)
```

With the validation set the model performs very well with an accuracy of 
~98%. It has very few values off the diagonal indicating good specificity
and sensitivity. Thus this model seems to do an excellent job of classifying
the quality of an exercise based on accelerometer data.

The predicted values for the test set are presented below. These were all 
correct as indicated by the quiz results.

```{r model predictions test set}
rfpred<-predict(rfmod,testpca)
rfpred
```